{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ed11b36-a374-4cff-8449-5f9d9dffbfcd",
   "metadata": {},
   "source": [
    "## Step 1: Load and Inspect Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2a79c3a-1176-427d-8723-4d4017a6bd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sample:\n",
      "   id  year  month  day  dep_time  sched_dep_time  dep_delay  arr_time  \\\n",
      "0   0  2013      1    1     517.0             515        2.0     830.0   \n",
      "1   1  2013      1    1     533.0             529        4.0     850.0   \n",
      "2   2  2013      1    1     542.0             540        2.0     923.0   \n",
      "3   3  2013      1    1     544.0             545       -1.0    1004.0   \n",
      "4   4  2013      1    1     554.0             600       -6.0     812.0   \n",
      "\n",
      "   sched_arr_time  arr_delay  ... flight  tailnum origin dest air_time  \\\n",
      "0             819       11.0  ...   1545   N14228    EWR  IAH    227.0   \n",
      "1             830       20.0  ...   1714   N24211    LGA  IAH    227.0   \n",
      "2             850       33.0  ...   1141   N619AA    JFK  MIA    160.0   \n",
      "3            1022      -18.0  ...    725   N804JB    JFK  BQN    183.0   \n",
      "4             837      -25.0  ...    461   N668DN    LGA  ATL    116.0   \n",
      "\n",
      "   distance  hour  minute            time_hour                    name  \n",
      "0      1400     5      15  2013-01-01 05:00:00   United Air Lines Inc.  \n",
      "1      1416     5      29  2013-01-01 05:00:00   United Air Lines Inc.  \n",
      "2      1089     5      40  2013-01-01 05:00:00  American Airlines Inc.  \n",
      "3      1576     5      45  2013-01-01 05:00:00         JetBlue Airways  \n",
      "4       762     6       0  2013-01-01 06:00:00    Delta Air Lines Inc.  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 336776 entries, 0 to 336775\n",
      "Data columns (total 21 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   id              336776 non-null  int64  \n",
      " 1   year            336776 non-null  int64  \n",
      " 2   month           336776 non-null  int64  \n",
      " 3   day             336776 non-null  int64  \n",
      " 4   dep_time        328521 non-null  float64\n",
      " 5   sched_dep_time  336776 non-null  int64  \n",
      " 6   dep_delay       328521 non-null  float64\n",
      " 7   arr_time        328063 non-null  float64\n",
      " 8   sched_arr_time  336776 non-null  int64  \n",
      " 9   arr_delay       327346 non-null  float64\n",
      " 10  carrier         336776 non-null  object \n",
      " 11  flight          336776 non-null  int64  \n",
      " 12  tailnum         334264 non-null  object \n",
      " 13  origin          336776 non-null  object \n",
      " 14  dest            336776 non-null  object \n",
      " 15  air_time        327346 non-null  float64\n",
      " 16  distance        336776 non-null  int64  \n",
      " 17  hour            336776 non-null  int64  \n",
      " 18  minute          336776 non-null  int64  \n",
      " 19  time_hour       336776 non-null  object \n",
      " 20  name            336776 non-null  object \n",
      "dtypes: float64(5), int64(10), object(6)\n",
      "memory usage: 54.0+ MB\n",
      "None\n",
      "Missing Values:\n",
      "id                   0\n",
      "year                 0\n",
      "month                0\n",
      "day                  0\n",
      "dep_time          8255\n",
      "sched_dep_time       0\n",
      "dep_delay         8255\n",
      "arr_time          8713\n",
      "sched_arr_time       0\n",
      "arr_delay         9430\n",
      "carrier              0\n",
      "flight               0\n",
      "tailnum           2512\n",
      "origin               0\n",
      "dest                 0\n",
      "air_time          9430\n",
      "distance             0\n",
      "hour                 0\n",
      "minute               0\n",
      "time_hour            0\n",
      "name                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data_path = \"./DataSets/KaggleData/flights.csv\"\n",
    "df_staging = pd.read_csv(data_path)\n",
    "\n",
    "# Initial inspection\n",
    "print(\"Data Sample:\")\n",
    "print(df_staging.head())\n",
    "print(\"Data Info:\")\n",
    "print(df_staging.info())\n",
    "print(\"Missing Values:\")\n",
    "print(df_staging.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4954df2f-0a15-4ba1-98f6-18e735177d7a",
   "metadata": {},
   "source": [
    "## Step 2: Data Quality Checks and Data Governance in Staging Area\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f41ab063-4419-4581-b3fb-2acb37773b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df_staging.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "# Remove duplicates and handle missing values\n",
    "df_staging = df_staging.drop_duplicates()\n",
    "df_staging = df_staging.dropna()  # Or apply other imputation methods as needed\n",
    "\n",
    "# Ensure data types are consistent and meaningful\n",
    "for column in df_staging.select_dtypes(include=['object']):\n",
    "    df_staging[column] = df_staging[column].astype(str).str.strip().str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b0bb8-5c7f-4d8e-bc33-9e3be1ee6118",
   "metadata": {},
   "source": [
    "## Step 3: Move Cleaned Data to Bronze Layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd6c94b6-c21e-4f76-990e-d7b4b0a4112b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze data saved at ./DataSets/KaggleData/bronze_flights.csv\n"
     ]
    }
   ],
   "source": [
    "# Saving cleaned data as bronze\n",
    "df_bronze = df_staging.copy()\n",
    "bronze_path = \"./DataSets/KaggleData/bronze_flights.csv\"\n",
    "df_bronze.to_csv(bronze_path, index=False)\n",
    "print(f\"Bronze data saved at {bronze_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48767a66-d707-4489-9608-fe5e2595684e",
   "metadata": {},
   "source": [
    "## Step 4: Data Transformation in Silver Layer\n",
    "* Apply transformations, e.g., converting timestamps, creating new columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa7d80e9-a09b-4ca3-bf0b-5b20faafe7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in the dataset:\n",
      "Index(['id', 'year', 'month', 'day', 'dep_time', 'sched_dep_time', 'dep_delay',\n",
      "       'arr_time', 'sched_arr_time', 'arr_delay', 'carrier', 'flight',\n",
      "       'tailnum', 'origin', 'dest', 'air_time', 'distance', 'hour', 'minute',\n",
      "       'time_hour', 'name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Display column names to verify available columns\n",
    "print(\"Available columns in the dataset:\")\n",
    "print(df_staging.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75c354a7-a556-45a4-a330-4206ad313e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrival and departure time columns not found. Please confirm column names.\n",
      "Departure time column not found. Please confirm column name.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming columns like 'scheduled_arrival' and 'scheduled_departure' might exist as alternatives\n",
    "df_silver = df_bronze.copy()\n",
    "\n",
    "# Example transformations, updated for actual columns\n",
    "if 'arrival_time' in df_silver.columns and 'departure_time' in df_silver.columns:\n",
    "    df_silver['flight_duration'] = (\n",
    "        pd.to_datetime(df_silver['arrival_time']) - pd.to_datetime(df_silver['departure_time'])\n",
    "    ).dt.total_seconds() / 60\n",
    "elif 'scheduled_arrival' in df_silver.columns and 'scheduled_departure' in df_silver.columns:\n",
    "    df_silver['flight_duration'] = (\n",
    "        pd.to_datetime(df_silver['scheduled_arrival']) - pd.to_datetime(df_silver['scheduled_departure'])\n",
    "    ).dt.total_seconds() / 60\n",
    "else:\n",
    "    print(\"Arrival and departure time columns not found. Please confirm column names.\")\n",
    "\n",
    "# Example to calculate day of the week for the departure date\n",
    "if 'departure_time' in df_silver.columns:\n",
    "    df_silver['day_of_week'] = pd.to_datetime(df_silver['departure_time']).dt.dayofweek\n",
    "elif 'scheduled_departure' in df_silver.columns:\n",
    "    df_silver['day_of_week'] = pd.to_datetime(df_silver['scheduled_departure']).dt.dayofweek\n",
    "else:\n",
    "    print(\"Departure time column not found. Please confirm column name.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed5ddfff-1136-4da2-b365-3fedc256b64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze layer data saved to ./DataSets/KaggleData/flights_bronze.csv\n",
      "Silver layer data saved to ./DataSets/KaggleData/flights_silver.csv\n",
      "Gold layer data saved to ./DataSets/KaggleData/flights_gold.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths for each layer\n",
    "bronze_path = \"./DataSets/KaggleData/flights_bronze.csv\"\n",
    "silver_path = \"./DataSets/KaggleData/flights_silver.csv\"\n",
    "gold_path = \"./DataSets/KaggleData/flights_gold.csv\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(\"./DataSets/KaggleData/\", exist_ok=True)\n",
    "\n",
    "# Load the initial data into the bronze layer\n",
    "df_bronze = pd.read_csv('./DataSets/KaggleData/flights.csv')\n",
    "df_bronze.to_csv(bronze_path, index=False)\n",
    "print(f\"Bronze layer data saved to {bronze_path}\")\n",
    "\n",
    "# Copy data from bronze layer to silver layer\n",
    "df_silver = df_bronze.copy()\n",
    "\n",
    "# Check if the relevant columns exist and perform transformations\n",
    "if 'arr_time' in df_silver.columns and 'dep_time' in df_silver.columns:\n",
    "    # Calculate flight duration (in minutes)\n",
    "    df_silver['flight_duration'] = (\n",
    "        pd.to_datetime(df_silver['arr_time']) - pd.to_datetime(df_silver['dep_time'])\n",
    "    ).dt.total_seconds() / 60\n",
    "else:\n",
    "    print(\"Arrival ('arr_time') and departure ('dep_time') columns not found. Please confirm column names.\")\n",
    "\n",
    "# Calculate day of the week for the departure time\n",
    "if 'dep_time' in df_silver.columns:\n",
    "    df_silver['day_of_week'] = pd.to_datetime(df_silver['dep_time']).dt.dayofweek\n",
    "else:\n",
    "    print(\"Departure time ('dep_time') column not found. Please confirm column name.\")\n",
    "\n",
    "# Check if delay columns exist and calculate them (if they exist)\n",
    "if 'arr_delay' in df_silver.columns and 'dep_delay' in df_silver.columns:\n",
    "    # Calculate total delay (in minutes)\n",
    "    df_silver['total_delay'] = df_silver['arr_delay'] + df_silver['dep_delay']\n",
    "else:\n",
    "    print(\"Arrival delay ('arr_delay') and departure delay ('dep_delay') columns not found. Please confirm column names.\")\n",
    "\n",
    "# Save the transformed data to the silver layer\n",
    "df_silver.to_csv(silver_path, index=False)\n",
    "print(f\"Silver layer data saved to {silver_path}\")\n",
    "\n",
    "# Additional transformation and data quality checks for gold layer (example: filtering out flights with excessive delays)\n",
    "df_gold = df_silver[df_silver['total_delay'] <= 120]  # Example: keep flights with <= 2-hour delay\n",
    "df_gold.to_csv(gold_path, index=False)\n",
    "print(f\"Gold layer data saved to {gold_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "130a8ac6-7768-498a-a6cb-8b1db09d94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_gold = df_silver.copy()\n",
    "\n",
    "# # Drop irrelevant or sensitive columns if any\n",
    "# df_gold = df_gold.drop(columns=['sensitive_column_name'])  # Modify based on data\n",
    "\n",
    "# # Save to gold layer\n",
    "# gold_path = \"./DataSets/KaggleData/gold_flights.csv\"\n",
    "# df_gold.to_csv(gold_path, index=False)\n",
    "# print(f\"Gold data saved at {gold_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23821c59-df2a-4667-bdfb-95b1db309373",
   "metadata": {},
   "source": [
    "## Step 6: Perform Predictive Analytics (e.g., Flight Delay Prediction)\n",
    "* Assuming a column \"delay\" exists, we predict whether a flight will be delayed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2cd3bf-dc27-4428-8576-96bb3b96a6fd",
   "metadata": {},
   "source": [
    "### Predicting Flight Delays Using Random Forest\n",
    "\n",
    "In this section, we prepare the flight dataset for machine learning by encoding categorical variables such as airline codes and flight routes into numerical values using Label Encoding. This is necessary for training the `RandomForestClassifier` model, which requires numerical input data. After encoding, we split the dataset into training and testing sets and use the trained model to predict flight delays. The performance is evaluated using accuracy and a classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "142db90d-cfbd-46c9-8b7d-cd6828ae520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9192245915258931\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.93     47835\n",
      "           1       0.95      0.87      0.91     42440\n",
      "\n",
      "    accuracy                           0.92     90275\n",
      "   macro avg       0.92      0.92      0.92     90275\n",
      "weighted avg       0.92      0.92      0.92     90275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load the final gold layer data\n",
    "df_gold = pd.read_csv('./DataSets/KaggleData/flights_gold.csv')\n",
    "\n",
    "# Ensure the delay column is created based on arrival and departure delay values\n",
    "df_gold['delay'] = ((df_gold['arr_delay'] > 0) | (df_gold['dep_delay'] > 0)).astype(int)\n",
    "\n",
    "# Prepare data by encoding categorical variables\n",
    "categorical_columns = df_gold.select_dtypes(include=['object']).columns\n",
    "label_encoders = {}\n",
    "\n",
    "for column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df_gold[column] = le.fit_transform(df_gold[column].astype(str))\n",
    "    label_encoders[column] = le  # Store encoders in case of reverse transformation later\n",
    "\n",
    "# Prepare data for modeling\n",
    "X = df_gold.drop(columns=['delay', 'arr_delay', 'dep_delay', 'arr_time', 'dep_time'])\n",
    "y = df_gold['delay']  # Target variable\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559b4ce-cef2-470b-88bd-e4648c703785",
   "metadata": {},
   "source": [
    "## Step 7: Bias Detection and Mitigation Using Fairlearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a10b66-c2ff-4028-ab2f-72eadede5464",
   "metadata": {},
   "source": [
    "### 3. Bias Detection with Fairlearn\n",
    "*We will check for bias based on the 'carrier' (as an example of sensitive attribute)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d705dfe-1bff-476e-8b71-36bcebf3efea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in dataset: Index(['id', 'year', 'month', 'day', 'dep_time', 'sched_dep_time', 'dep_delay',\n",
      "       'arr_time', 'sched_arr_time', 'arr_delay', 'carrier', 'flight',\n",
      "       'tailnum', 'origin', 'dest', 'air_time', 'distance', 'hour', 'minute',\n",
      "       'time_hour', 'name', 'flight_duration', 'day_of_week', 'total_delay'],\n",
      "      dtype='object')\n",
      "Unique target values in 'dest': ['IAH' 'MIA' 'BQN' 'ATL' 'ORD' 'FLL' 'IAD' 'MCO' 'PBI' 'TPA' 'LAX' 'SFO'\n",
      " 'DFW' 'BOS' 'LAS' 'MSP' 'DTW' 'RSW' 'SJU' 'PHX' 'BWI' 'CLT' 'BUF' 'DEN'\n",
      " 'SNA' 'MSY' 'SLC' 'XNA' 'MKE' 'SEA' 'ROC' 'SYR' 'SRQ' 'RDU' 'CMH' 'JAX'\n",
      " 'CHS' 'MEM' 'PIT' 'SAN' 'DCA' 'CLE' 'STL' 'MYR' 'JAC' 'MDW' 'HNL' 'BNA'\n",
      " 'AUS' 'BTV' 'PHL' 'STT' 'EGE' 'AVL' 'PWM' 'IND' 'CAK' 'HOU' 'LGB' 'DAY'\n",
      " 'ALB' 'BDL' 'MHT' 'MSN' 'GSO' 'CVG' 'GSP' 'GRR' 'MCI' 'ORF' 'SAT' 'PDX'\n",
      " 'SJC' 'CRW' 'RIC' 'OAK' 'SMF' 'OMA' 'TYS' 'SDF' 'PVD' 'PSE' 'SAV' 'BUR'\n",
      " 'DSM' 'TUL' 'BHM' 'CAE' 'OKC' 'HDN' 'BZN' 'MTJ' 'EYW' 'PSP' 'ACK' 'ABQ'\n",
      " 'ILM' 'BGR' 'MVY' 'SBN' 'LEX' 'CHO' 'TVC' 'ANC']\n",
      "Unmapped target values found. Removing them.\n",
      "Demographic Parity Difference after mitigation: 0.0\n",
      "Mean Absolute Error after mitigation: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "from fairlearn.metrics import demographic_parity_difference\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Load dataset\n",
    "data_path = './DataSets/KaggleData/flights_gold.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Print available columns to verify\n",
    "print(\"Available columns in dataset:\", df.columns)\n",
    "\n",
    "# Inspect the unique values in the target column\n",
    "print(\"Unique target values in 'dest':\", df['dest'].unique())\n",
    "\n",
    "# Map target to binary values\n",
    "# Adjust mapping according to your actual target values\n",
    "binary_mapping = {'EWR': 0, 'SFO': 1}  # Example mapping; adjust as necessary\n",
    "df['target_binary'] = df['dest'].map(binary_mapping)\n",
    "\n",
    "# Check for unmapped values (NaN)\n",
    "if df['target_binary'].isnull().sum() > 0:\n",
    "    print(\"Unmapped target values found. Removing them.\")\n",
    "    df = df.dropna(subset=['target_binary'])\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['id', 'dest', 'target_binary'])  # Adjust feature selection as needed\n",
    "y = df['target_binary']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode categorical features (if any)\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "X_test_encoded = encoder.transform(X_test)\n",
    "\n",
    "# Define the sensitive feature\n",
    "sensitive_feature = 'carrier'  # Replace with your sensitive feature column name\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded)\n",
    "X_train_encoded[sensitive_feature] = X_train[sensitive_feature].values\n",
    "\n",
    "# Fit the mitigator using binary labels\n",
    "mitigator = ExponentiatedGradient(estimator=LinearRegression(),\n",
    "                                  constraints=DemographicParity())\n",
    "mitigator.fit(X_train_encoded, y_train, sensitive_features=X_train[sensitive_feature])\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_mitigated = mitigator.predict(X_test_encoded)\n",
    "\n",
    "# Assess fairness and performance\n",
    "dp_diff = demographic_parity_difference(y_test, y_pred_mitigated, sensitive_features=X_test[sensitive_feature])\n",
    "mae = mean_absolute_error(y_test, y_pred_mitigated)\n",
    "\n",
    "print(\"Demographic Parity Difference after mitigation:\", dp_diff)\n",
    "print(\"Mean Absolute Error after mitigation:\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4642ae05-1e04-42a7-b273-1a338464e387",
   "metadata": {},
   "source": [
    "### Explanation of the Code\n",
    "\n",
    "This code aims to:\n",
    "1. Build a **predictive model** for a binary classification task.\n",
    "2. **Mitigate bias** related to a sensitive feature (`carrier` in this case) using Fairlearn's fairness reduction techniques.\n",
    "3. Assess both the **fairness** and **performance** of the model.\n",
    "\n",
    "### Detailed Steps\n",
    "\n",
    "#### 1. **Data Loading and Inspection**\n",
    "```python\n",
    "data_path = './DataSets/KaggleData/flights_gold.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "print(\"Available columns in dataset:\", df.columns)\n",
    "```\n",
    "- The dataset is loaded from the specified path.\n",
    "- The columns are printed to verify the structure of the data and ensure that all required columns are present.\n",
    "\n",
    "#### 2. **Target Transformation**\n",
    "```python\n",
    "print(\"Unique target values in 'dest':\", df['dest'].unique())\n",
    "binary_mapping = {'EWR': 0, 'SFO': 1}\n",
    "df['target_binary'] = df['dest'].map(binary_mapping)\n",
    "```\n",
    "- The target column (`dest`) represents flight destinations. It's converted into a binary variable:\n",
    "  - `EWR` is mapped to `0`\n",
    "  - `SFO` is mapped to `1`\n",
    "- If any rows have destinations not mapped, those rows are dropped:\n",
    "```python\n",
    "if df['target_binary'].isnull().sum() > 0:\n",
    "    df = df.dropna(subset=['target_binary'])\n",
    "```\n",
    "\n",
    "#### 3. **Feature Selection**\n",
    "```python\n",
    "X = df.drop(columns=['id', 'dest', 'target_binary'])\n",
    "y = df['target_binary']\n",
    "```\n",
    "- The features (`X`) are created by dropping irrelevant or target-related columns (`id`, `dest`, `target_binary`).\n",
    "- The target (`y`) is the binary-transformed column.\n",
    "\n",
    "#### 4. **Data Splitting**\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "- The dataset is split into training (80%) and testing (20%) subsets.\n",
    "\n",
    "#### 5. **Categorical Encoding**\n",
    "```python\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "X_test_encoded = encoder.transform(X_test)\n",
    "```\n",
    "- Categorical features in `X` are encoded into numerical values using one-hot encoding.\n",
    "- One-hot encoding ensures that each category is represented as a binary column.\n",
    "\n",
    "#### 6. **Bias Mitigation with Fairlearn**\n",
    "```python\n",
    "sensitive_feature = 'carrier'\n",
    "X_train_encoded = pd.DataFrame(X_train_encoded)\n",
    "X_train_encoded[sensitive_feature] = X_train[sensitive_feature].values\n",
    "```\n",
    "- `carrier` is identified as the **sensitive feature** (e.g., airline operator).\n",
    "- This column is appended back to the encoded training data.\n",
    "\n",
    "#### 7. **Training the Fairness-Aware Model**\n",
    "```python\n",
    "mitigator = ExponentiatedGradient(estimator=LinearRegression(),\n",
    "                                  constraints=DemographicParity())\n",
    "mitigator.fit(X_train_encoded, y_train, sensitive_features=X_train[sensitive_feature])\n",
    "```\n",
    "- An **Exponentiated Gradient** mitigator is used to enforce **Demographic Parity** while training a `LinearRegression` model:\n",
    "  - **Demographic Parity** ensures that the prediction is independent of the sensitive feature, meaning the likelihood of predicting `1` is similar across different groups defined by `carrier`.\n",
    "\n",
    "#### 8. **Predictions and Evaluation**\n",
    "```python\n",
    "y_pred_mitigated = mitigator.predict(X_test_encoded)\n",
    "dp_diff = demographic_parity_difference(y_test, y_pred_mitigated, sensitive_features=X_test[sensitive_feature])\n",
    "mae = mean_absolute_error(y_test, y_pred_mitigated)\n",
    "```\n",
    "- Predictions are made using the fairness-aware model.\n",
    "- Two key metrics are calculated:\n",
    "  1. **Demographic Parity Difference (`dp_diff`)**: Measures fairness by comparing the prediction rates across groups. A lower value indicates better fairness.\n",
    "  2. **Mean Absolute Error (`mae`)**: Evaluates the model's prediction accuracy.\n",
    "\n",
    "#### 9. **Results**\n",
    "```python\n",
    "print(\"Demographic Parity Difference after mitigation:\", dp_diff)\n",
    "print(\"Mean Absolute Error after mitigation:\", mae)\n",
    "```\n",
    "- The results display:\n",
    "  - How well the model performed on fairness (low `dp_diff` is better).\n",
    "  - How accurate the model is (low `mae` is better).\n",
    "\n",
    "---\n",
    "\n",
    "### Goal of the Code\n",
    "- **Bias Mitigation**: Train a model that reduces unfair advantages/disadvantages for groups defined by the sensitive feature (`carrier`).\n",
    "- **Model Accuracy**: Balance fairness with prediction accuracy.\n",
    "- **Fairness Metric**: Use **Demographic Parity Difference** to measure fairness improvement.\n",
    "\n",
    "This pipeline is particularly useful for analyzing and mitigating bias in real-world datasets where fairness is a priority (e.g., hiring, lending, or resource allocation). Let me know if you'd like clarification on specific parts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e6a7e-9681-419e-8e95-dcc1ee1e60c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8b95e-a5e7-41c0-941e-799c9eeb0cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
