{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7c2ab5-73f6-466d-a954-fdb6333643e8",
   "metadata": {},
   "source": [
    "Here's a comprehensive approach to build a data pipeline for your flight data file using Python. This pipeline will implement the Data Medallion Architecture, consisting of staging (raw), bronze (cleaned), silver (transformed), and gold (ready-for-analysis) layers. The pipeline will also check data quality and governance, perform predictive analytics, and mitigate bias where possible.\n",
    "\n",
    "### Required Libraries\n",
    "\n",
    "To execute this code, ensure you have the following libraries installed:\n",
    "```bash\n",
    "pip install pandas numpy sklearn fairlearn transformers torch\n",
    "```\n",
    "\n",
    "### Step 1: Load and Inspect Data\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data_path = \"/mnt/data/flights.csv\"\n",
    "df_staging = pd.read_csv(data_path)\n",
    "\n",
    "# Initial inspection\n",
    "print(\"Data Sample:\")\n",
    "print(df_staging.head())\n",
    "print(\"Data Info:\")\n",
    "print(df_staging.info())\n",
    "print(\"Missing Values:\")\n",
    "print(df_staging.isnull().sum())\n",
    "```\n",
    "\n",
    "### Step 2: Data Quality Checks and Data Governance in Staging Area\n",
    "\n",
    "```python\n",
    "# Check for duplicates\n",
    "duplicates = df_staging.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "# Remove duplicates and handle missing values\n",
    "df_staging = df_staging.drop_duplicates()\n",
    "df_staging = df_staging.dropna()  # Or apply other imputation methods as needed\n",
    "\n",
    "# Ensure data types are consistent and meaningful\n",
    "for column in df_staging.select_dtypes(include=['object']):\n",
    "    df_staging[column] = df_staging[column].astype(str).str.strip().str.lower()\n",
    "```\n",
    "\n",
    "### Step 3: Move Cleaned Data to Bronze Layer\n",
    "\n",
    "```python\n",
    "# Saving cleaned data as bronze\n",
    "df_bronze = df_staging.copy()\n",
    "bronze_path = \"/mnt/data/bronze_flights.csv\"\n",
    "df_bronze.to_csv(bronze_path, index=False)\n",
    "print(f\"Bronze data saved at {bronze_path}\")\n",
    "```\n",
    "\n",
    "### Step 4: Data Transformation in Silver Layer\n",
    "\n",
    "Apply transformations, e.g., converting timestamps, creating new columns.\n",
    "\n",
    "```python\n",
    "# Feature engineering for silver layer\n",
    "df_silver = df_bronze.copy()\n",
    "\n",
    "# Example transformations\n",
    "df_silver['flight_duration'] = (pd.to_datetime(df_silver['arrival_time']) - pd.to_datetime(df_silver['departure_time'])).dt.total_seconds() / 60\n",
    "df_silver['day_of_week'] = pd.to_datetime(df_silver['departure_time']).dt.dayofweek\n",
    "\n",
    "# Save to silver layer\n",
    "silver_path = \"/mnt/data/silver_flights.csv\"\n",
    "df_silver.to_csv(silver_path, index=False)\n",
    "print(f\"Silver data saved at {silver_path}\")\n",
    "```\n",
    "\n",
    "### Step 5: Prepare Gold Data Layer (Finalized Data for Analysis)\n",
    "\n",
    "Apply further cleaning and final adjustments.\n",
    "\n",
    "```python\n",
    "df_gold = df_silver.copy()\n",
    "\n",
    "# Drop irrelevant or sensitive columns if any\n",
    "df_gold = df_gold.drop(columns=['sensitive_column_name'])  # Modify based on data\n",
    "\n",
    "# Save to gold layer\n",
    "gold_path = \"/mnt/data/gold_flights.csv\"\n",
    "df_gold.to_csv(gold_path, index=False)\n",
    "print(f\"Gold data saved at {gold_path}\")\n",
    "```\n",
    "\n",
    "### Step 6: Perform Predictive Analytics (e.g., Flight Delay Prediction)\n",
    "\n",
    "Assuming a column \"delay\" exists, we predict whether a flight will be delayed.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Prepare data for modeling\n",
    "X = df_gold.drop(columns=['delay'])\n",
    "y = df_gold['delay']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "### Step 7: Bias Detection and Mitigation Using Fairlearn\n",
    "\n",
    "Check for bias related to sensitive features (e.g., location, airline).\n",
    "\n",
    "```python\n",
    "from fairlearn.metrics import MetricFrame, false_positive_rate, selection_rate\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "\n",
    "# Define sensitive feature\n",
    "sensitive_feature = 'location'\n",
    "\n",
    "# Evaluate bias in model predictions\n",
    "metric_frame = MetricFrame(metrics={\"selection_rate\": selection_rate, \"fpr\": false_positive_rate},\n",
    "                           y_true=y_test, y_pred=y_pred, sensitive_features=X_test[sensitive_feature])\n",
    "\n",
    "print(\"Bias metrics:\")\n",
    "print(metric_frame.by_group)\n",
    "\n",
    "# Bias mitigation\n",
    "mitigator = ExponentiatedGradient(model, constraints=DemographicParity())\n",
    "mitigator.fit(X_train, y_train, sensitive_features=X_train[sensitive_feature])\n",
    "\n",
    "# Predict with mitigated model\n",
    "y_pred_mitigated = mitigator.predict(X_test)\n",
    "print(\"Mitigated Model Accuracy:\", accuracy_score(y_test, y_pred_mitigated))\n",
    "print(\"Mitigated Classification Report:\\n\", classification_report(y_test, y_pred_mitigated))\n",
    "```\n",
    "\n",
    "### Step 8: Generate a Report Using a Language Model\n",
    "\n",
    "Using Hugging Face's language model to generate a report summarizing findings:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load language model for text generation\n",
    "report_generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-2.7B\")\n",
    "\n",
    "# Prompt for report generation\n",
    "report_prompt = f\"\"\"\n",
    "Generate a report on the flight data analysis and predictive modeling results:\n",
    "\n",
    "1. Data Overview: Summary of initial data quality issues and transformations applied in the bronze and silver layers.\n",
    "2. Predictive Analysis Results: Overview of the model used, its performance, and accuracy in predicting flight delays.\n",
    "3. Bias Analysis and Mitigation: Describe the detected biases and how the bias mitigation techniques improved model fairness.\n",
    "4. Key Recommendations: Based on the findings, suggest steps to further improve data quality, modeling accuracy, and fairness.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Generate report\n",
    "report = report_generator(report_prompt, max_length=1024, num_return_sequences=1)\n",
    "print(\"Generated Report:\\n\", report[0][\"generated_text\"])\n",
    "```\n",
    "\n",
    "### Summary of the Pipeline:\n",
    "1. **Staging Area**: Load raw data, perform initial cleaning, and validate data.\n",
    "2. **Bronze Layer**: Save deduplicated and cleansed data.\n",
    "3. **Silver Layer**: Perform feature engineering and further transformations.\n",
    "4. **Gold Layer**: Save the final dataset, ready for analysis.\n",
    "5. **Predictive Analytics**: Train a model to predict delays.\n",
    "6. **Bias Detection and Mitigation**: Check and address bias using Fairlearn.\n",
    "7. **Report Generation**: Summarize findings using an LLM.\n",
    "\n",
    "This pipeline provides a comprehensive foundation for data quality, transformation, analytics, and reporting within the Medallion Architecture framework. Adjust specific column names and transformations based on the exact structure of your `flights.csv` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1119fb-7c89-45fe-b8be-48802679ce83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
